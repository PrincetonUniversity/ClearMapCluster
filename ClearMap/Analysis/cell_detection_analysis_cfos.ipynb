{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClearMap cell detection Analysis Tutorial, Part 1.\n",
    "\n",
    "### This is a brief tutorial designed to analyze detected c-Fos+ cells among behavioral groups. \n",
    "It outlines the basic statistics you may want to run for an experiment in which you have collected and cleared brains after a behavior and stained for c-Fos. \n",
    "#### Note: It assumes you have processed and done quality control on brain volumes processed through T.Pisano/Z.Dhanerawala's package, ClearMapCluster. \n",
    "Before beginning the tutorial, check that each brain in your processed directory has a csv file named `\"Annotated_counts_[..]\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant modules\n",
    "import os, numpy as np, pandas as pd, scipy, itertools, sys, json, seaborn as sns\n",
    "import matplotlib.pyplot as plt, matplotlib as mpl\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "#these are formatting settings useful for importing the exported plots into image editing software later (Illustrator)\n",
    "mpl.rcParams[\"pdf.fonttype\"] = 42\n",
    "mpl.rcParams[\"ps.fonttype\"] = 42\n",
    "mpl.rcParams[\"xtick.major.size\"] = 6\n",
    "mpl.rcParams[\"ytick.major.size\"] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions\n",
    "\n",
    "Below we are going to define 2 key functions used to do analysis: one, to traverse through the Allen ontology and find parent and child structures for each brain region, and another to compile the composite dataframe using the individual dataframes generated per brain. \n",
    "\n",
    "#### Note: You will generally not have to modify these functions unless you are trying to troubleshoot an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_progeny(dic,parent_structure,progeny_list):\n",
    "    \"\"\" \n",
    "    ---PURPOSE---\n",
    "    Get a list of all progeny of a structure name.\n",
    "    This is a recursive function which is why progeny_list is an\n",
    "    argument and is not returned.\n",
    "    ---INPUT---\n",
    "    dic                  A dictionary representing the JSON file \n",
    "                         which contains the ontology of interest\n",
    "    parent_structure     The structure\n",
    "    progeny_list         The list to which this function will \n",
    "                         append the progeny structures. \n",
    "    \"\"\"\n",
    "    if 'msg' in list(dic.keys()): dic = dic['msg'][0]\n",
    "    \n",
    "    name = dic.get(\"name\")\n",
    "    children = dic.get(\"children\")\n",
    "    if name == parent_structure:\n",
    "        for child in children: # child is a dict\n",
    "            child_name = child.get(\"name\")\n",
    "            progeny_list.append(child_name)\n",
    "            get_progeny(child,parent_structure=child_name,progeny_list=progeny_list)\n",
    "        return\n",
    "    \n",
    "    for child in children:\n",
    "        child_name = child.get(\"name\")\n",
    "        get_progeny(child,parent_structure=parent_structure,progeny_list=progeny_list)\n",
    "    return \n",
    "\n",
    "def generate_data_frame(conditions, lst, dst, flnm, \n",
    "                        exclude, ontology_file, scale = 0.025):\n",
    "    \"\"\" \n",
    "    used to make a pooled csv file of all cell counts in an experiment\n",
    "    inputs:\n",
    "        conditions: zip of file names + condition\n",
    "        lst: list of file names run through analysis\n",
    "        dst: path to save csv output\n",
    "        flnm: the filename of the brain-specific csv files without the .csv extension (e.g. 'annotated_counts')\n",
    "        exclude: list of allen structures to exlude from analysis\n",
    "        ontology_file: path to allen json file\n",
    "        scale: atlas scale, default 25 um/voxel (0.025 mm/voxel); change if using a different atlas resolution\n",
    "    \"\"\"\n",
    "    #open ontology file\n",
    "    with open(ontology_file) as json_file:\n",
    "        ontology_dict = json.load(json_file)\n",
    "\n",
    "    #generate data frame\n",
    "    bglst=[]\n",
    "    for fl in lst:\n",
    "        #extract out info\n",
    "        nm = os.path.basename(fl)\n",
    "        #make dataframe\n",
    "        df = pd.read_csv(fl+\"/%s.csv\" % flnm)[1:] #remove previous headers\n",
    "        print(nm, df.shape)\n",
    "        df = df.replace(np.nan, \"\", regex=True)\n",
    "        df[\"Brain\"] = nm\n",
    "        df[\"Condition\"] = conditions[nm]\n",
    "        \n",
    "        #sum up ontology for each structure\n",
    "        for struct in df.name.values:\n",
    "            progeny = []; counts = []\n",
    "            get_progeny(ontology_dict, struct, progeny)\n",
    "            #add structure counts to itself\n",
    "            counts.append(df[df.name == struct][\"counts\"].values[0])\n",
    "            for progen in progeny:\n",
    "                try:\n",
    "                    counts.append(df[df.name == progen][\"counts\"].values[0])\n",
    "                except:\n",
    "                    counts.append(0)\n",
    "            df.loc[df.name == struct, \"counts\"] = np.sum(np.array(counts))\n",
    "        \n",
    "        #remove structures you don't want to analyze\n",
    "        for soi in exclude:\n",
    "            df = df[df.name != soi]\n",
    "            progeny = []; get_progeny(ontology_dict, soi, progeny)\n",
    "            for progen in progeny:\n",
    "                df = df[df.name != progen]\n",
    "        \n",
    "        #append to composite dataframe\n",
    "        bglst.append(df)\n",
    "\n",
    "    df = pd.concat(bglst)\n",
    "    df[\"counts\"] = df[\"counts\"].apply(int)\n",
    "\n",
    "    df = df.drop(columns = [\"Unnamed: 0\"])\n",
    "    #get all brain names\n",
    "    brains = np.unique(df.Brain.values)\n",
    "    \n",
    "    #save total counts in dict\n",
    "    total_counts = {}\n",
    "    percents = {}\n",
    "    #for each brain, get total counts\n",
    "    for brain in brains:\n",
    "        total_counts[brain] = df[df.Brain == brain].counts.sum(0)               \n",
    "    percents = [df[df.Brain == brain].counts.apply(lambda x: (x/total_counts[brain])*100).astype(\"float64\") for brain in brains]\n",
    "    #concantenate together\n",
    "    df_percent = pd.concat(percents)\n",
    "    df[\"percent\"] = df_percent\n",
    "    df[\"density\"] = df[df.voxels_in_structure > 0].apply(lambda x:x.counts/(float(x.voxels_in_structure*(scale**3))), 1)\n",
    "    \n",
    "    df.to_csv(os.path.join(dst, \"cell_counts.csv\"), index = None)\n",
    "    \n",
    "    return os.path.join(dst, \"cell_counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting paths\n",
    "\n",
    "Below we are going to define variables and paths needed to import and export our analysis dataframe. Generally these will be system dependent, and experiment dependent, so make sure you modify these accordingly.\n",
    "\n",
    "#### Note: For Windows OS, the path string will look different. Make sure you check and set the paths according to what makes sense for your OS.\n",
    "E.g. In Windows, if you have mounted a file server under disk name \"H:\", your path to the `LightSheetData` fileshare should look something like this:\n",
    "`df_pth = r\"H:\\falkner-mouse\\allen_atlas\\allen_id_table_w_voxel_counts.xlsx\"`.\n",
    "\n",
    "The \"r\" in front of the path is required for Windows string formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set appropriate paths\n",
    "df_pth = \"/jukebox/LightSheetData/falkner-mouse/allen_atlas/allen_id_table_w_voxel_counts.xlsx\"\n",
    "ann_pth = \"/jukebox/LightSheetData/falkner-mouse/allen_atlas/annotation_2017_25um_sagittal_forDVscans.nrrd\"\n",
    "atl_pth = \"/jukebox/LightSheetData/falkner-mouse/allen_atlas/average_template_25_sagittal_forDVscans.tif\"\n",
    "ontology_file = \"/jukebox/LightSheetData/falkner-mouse/allen_atlas/allen.json\"\n",
    "src = \"/jukebox/LightSheetData/falkner-mouse/scooter/clearmap_processed\"\n",
    "dst = \"/jukebox/LightSheetData/falkner-mouse/scooter/pooled_analysis\" #where all output will be saved\n",
    "if not os.path.exists(dst): os.mkdir(dst)\n",
    "\n",
    "#get files that have annotated counts per brain\n",
    "lst = [os.path.join(src, fld) for fld in os.listdir(src) \n",
    "       if os.path.exists(os.path.join(os.path.join(src, fld), \n",
    "                                                     \"Annotated_counts_60um_edge_80um_vntric_erosion.csv\"))]; lst.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to define animal names and condition variables based on your experimental paradigm. Typically you would have all your processed brains organized in one directory, and each brain name would correspond to a behavioral condition:\n",
    "E.g. `mfnp3` was part of the `male-female` behavior group. \n",
    "\n",
    "Using this info, you can define the animals and corresponding behavioral conditions they belong to in the 2 lists below. This will be useful in making the composite dataframe and running statistical tests later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conditions based on experiment - SET AND DOUBLE CHECK THESE MANUALLY\n",
    "nms = [\"mfnp3\", \"fmnp4\", \"mfnp2\", \"mmnp4\", \"mmnp6\", \n",
    "       \"mmnp5\", \"fmnp6\", \"fmnp5\"]\n",
    "#above animal name position should correspond to condition\n",
    "cond = [\"male-female\", \"female-male\", \"male-female\", \"male-male\", \"male-male\",\n",
    "        \"male-male\", \"female-male\", \"female-male\"]\n",
    "conditions = {n:c for n,c in zip(nms, cond)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find brain regions that have elevated c-Fos+ cells, you will be comparing across the whole brain. Optionally, you can chose to name regions defined in the [Allen ontology](http://atlas.brain-map.org/atlas?atlas=2&plate=100883867) that you would like to exclude from these comparisons. Here we exclude ventricles and white matter tracts as these do not have neurons in the mouse brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name structures you want to exclude from analysis\n",
    "exclude = [\"ventricular systems\", \"fiber tracts\", \"grooves\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make composite dataframe\n",
    "\n",
    "Now we can run the function to make the composite dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_pth = generate_data_frame(conditions, lst, dst, \"Annotated_counts_60um_edge_80um_vntric_erosion\", exclude, ontology_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print out a portion of the generated dataframe to see what it looks like, or inspect it in Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_pth)\n",
    "df.head(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "Then, we can run basic, group-wise statistics on the behavioral conditions in this experiment using the `condition` list defined earlier. We can start with a one-way ANOVA with a correction for multiple comparisons (Tukey HSD)\n",
    "\n",
    "You can define the p-value cutoff you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set the measure you want to use for your statistics (cell density, cell count, normalized cell count, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#measure to use for tests\n",
    "measure = \"density\" #you can change this to count, or percent later "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the statistics for each structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv generated above\n",
    "df = pd.read_csv(csv_pth, index_col = None)\n",
    "\n",
    "#formatting to include structure name and parent name\n",
    "df_anova = pd.DataFrame()\n",
    "df_anova[\"name\"] = df.name.unique()\n",
    "df_anova[\"parent_name\"] = [df.loc[df.name == nm, \"parent_name\"].values[0]\n",
    "                           for nm in df_anova[\"name\"]]\n",
    "\n",
    "for nm in np.unique(df.name.values): #to run on each structure\n",
    "    f, pval = f_oneway(df[(df.name == nm) & (df.Condition == \"male-male\")][measure].values, \n",
    "                 df[(df.name == nm) & (df.Condition == \"female-male\")][measure].values,\n",
    "                 df[(df.name == nm) & (df.Condition == \"male-female\")][measure].values)\n",
    "\n",
    "    df_anova.loc[(df_anova[\"name\"] == nm), \"anova_f\"] = f\n",
    "    df_anova.loc[(df_anova[\"name\"] == nm), \"anova_pval\"] = pval\n",
    "\n",
    "df_anova.to_csv(os.path.join(dst, \"one_way_anova_{}.csv\".format(measure)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the results of the ANOVA to visualize the significant structures per group, using seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find structures with multiple comparisons corrections    \n",
    "sig_structs = df_anova[df_anova.anova_pval < 0.05].name.values\n",
    "#plot the ones with lower density separately\n",
    "sig_structures_ld = [xx for xx in sig_structs \n",
    "                     if df[df.name == xx][measure].mean() < 100]\n",
    "\n",
    "df_nm = df[(df.name.isin(sig_structs)) & (~df.name.isin(sig_structures_ld))]\n",
    "fig, ax = plt.subplots(figsize=(5,10))\n",
    "sns.stripplot(y = \"name\", x = measure, hue = \"Condition\", data = df_nm,\n",
    "            orient = \"h\", size=7)\n",
    "#you can also choose to uncomment the lines below and make boxplots instead\n",
    "# sns.boxplot(y = \"name\", x = \"density\", hue = \"Condition\", data = df_nm,\n",
    "#             orient = \"h\", showfliers=False, showcaps=False, \n",
    "#         boxprops={'facecolor':'None'})\n",
    "plt.xlabel(\"Density (cells/mm$^3$)\")\n",
    "plt.ylabel(\"Structure\")\n",
    "#save out\n",
    "plt.savefig(os.path.join(dst, \"boxplots_%s.pdf\" % measure), dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make a separate plot for structures with lower densities that are too small to visualize on the above X-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld = df[df.name.isin(sig_structures_ld)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,2))\n",
    "sns.stripplot(y = \"name\", x = measure, hue = \"Condition\", data = df_ld,\n",
    "            orient = \"h\", size=7)\n",
    "\n",
    "plt.xlabel(\"Density (cells/mm$^3$)\")\n",
    "plt.ylabel(\"Structure\")\n",
    "#save out\n",
    "plt.savefig(os.path.join(dst, \"boxplots_lower_%s.pdf\") % measure, \n",
    "            dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also many ways you can customize our plots for visualization, with color and labels. You can look up the parameters in seaborn and matplotlib in python for further info. \n",
    "\n",
    "E.g. You can change the color of your datapoints using `palette`, and adjust the size of your data points using `size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,2))\n",
    "sns.stripplot(y = \"name\", x = measure, hue = \"Condition\", data = df_ld,\n",
    "            orient = \"h\", size=5, palette=[\"black\", \"red\", \"blue\"])\n",
    "\n",
    "plt.xlabel(\"Density (cells/mm$^3$)\")\n",
    "plt.ylabel(\"Structure\")\n",
    "plt.savefig(os.path.join(dst, \"boxplots_lower_%s.pdf\") % measure, \n",
    "            dpi = 300, bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversing the Allen hierarchy\n",
    "\n",
    "The [Allen Mouse Brain Atlas](http://atlas.brain-map.org/atlas?atlas=2&plate=100883867) is organized into a hierarchy of structures. In the interactive website, you can use the pull down menu to look at various structure names and the \"parent\" they belong to (e.g. Somatomotor areas belong to the Isocortex). This hierarchy is organized into a hierarchically-structure `.json` file that is downloadable from the Allen Brain Institute API. \n",
    "\n",
    "This `.json` file is useful if you want to analyze larger \"parent\" structures that are represented by several child structures in the 3D annotation file. You can simply sum up all the cells detected in the individual child structures into a parent structure, add that to your dataframe, and perform statistics on those instead. This is especially useful for small structures - such as neocortical layers, or subnuclei - that maybe too small to be reliably used for cell detection and statistical analysis. This will depend on the resolution of your images and the experiemntal paradigm. \n",
    "\n",
    "#### Making a new dataframe with summed parent structures\n",
    "\n",
    "Below is an example on how to sum up all the layers of the neocortical areas, and use those summed areas for the one-way ANOVA instead. All you need to do is make a one-column list of structures you want to sum up in Excel and save it as a `.csv`. Here is my example `.csv.`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read my structure list\n",
    "sum_pth = \"/jukebox/LightSheetData/falkner-mouse/allen_atlas/structures.csv\" #change path if necessary\n",
    "sum_df = pd.read_csv(sum_pth, header=None)\n",
    "\n",
    "print(sum_df.head(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note above that I have only included the Allen structures I would like to sum up, and nothing else. The rest of the structures in the atlas (at the lowest level of the hierarchy) will be kept automatically in further analysis.\n",
    "\n",
    "We can now extract the list of the structure names *and* their child structures, which will be useful in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of summed structures\n",
    "with open(ontology_file) as json_file:\n",
    "        ontology_dict = json.load(json_file)\n",
    "        \n",
    "sum_str = [xx[0] for xx in sum_df.values]\n",
    "sum_str_progeny = []\n",
    "for strc in sum_str:\n",
    "    progeny = []\n",
    "    get_progeny(ontology_dict, strc, progeny)\n",
    "    for progen in progeny:\n",
    "        sum_str_progeny.append(progen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then redefine the `generate_data_frame()` function to include functionality that takes the larger parent structures in our list, sums all their child structures, and adds them to the composite `cell_counts.csv` dataframe, which we will now export to `cell_counts_summed.csv`.\n",
    "\n",
    "#### Note: You will generally not have to modify this function unless you are trying to troubleshoot an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summed_data_frame(conditions, lst, pth, flnm, \n",
    "                        exclude, ontology_file, sum_str, sum_str_progeny,scale = 0.025):\n",
    "    \"\"\" \n",
    "    used to make a pooled csv file of all cell counts in an experiment\n",
    "    inputs:\n",
    "        conditions: zip of file names + condition\n",
    "        lst: list of file names run through analysis\n",
    "        pth: path to save csv output\n",
    "        flnm: the filename of the brain-specific csv files without the .csv extension (e.g. 'annotated_counts')\n",
    "        exclude: list of allen structures to exlude from analysis\n",
    "        ontology_file: path to allen json file\n",
    "        sum_str: structures to sum\n",
    "        sum_str_progeny: child structures of structures to sum\n",
    "        scale: atlas scale, default 25 um/voxel (0.025 mm/voxel); change if using a different atlas resolution\n",
    "    \"\"\"\n",
    "    #open ontology file\n",
    "    with open(ontology_file) as json_file:\n",
    "        ontology_dict = json.load(json_file)\n",
    "\n",
    "    #generate data frame\n",
    "    bglst=[]\n",
    "    for fl in lst:\n",
    "        #extract out info\n",
    "        nm = os.path.basename(fl)\n",
    "        #make dataframe\n",
    "        df = pd.read_csv(fl+\"/%s.csv\" % flnm)[1:] #remove previous headers\n",
    "        print(nm, df.shape)\n",
    "        \n",
    "        #make sure df doesnt have any summed structure names\n",
    "        df = df[~df.name.isin(sum_str)]\n",
    "        #sum up ontology for each structure\n",
    "        for struct in df.name.values:\n",
    "            progeny = []; counts = []\n",
    "            get_progeny(ontology_dict, struct, progeny)\n",
    "            #add structure counts to itself\n",
    "            counts.append(df[df.name == struct][\"counts\"].values[0])\n",
    "            for progen in progeny:\n",
    "                try:\n",
    "                    counts.append(df[df.name == progen][\"counts\"].values[0])\n",
    "                except:\n",
    "                    counts.append(0)\n",
    "            df.loc[df.name == struct, \"counts\"] = np.sum(np.array(counts))\n",
    "        #add summed structures\n",
    "        for struct in sum_str:\n",
    "            #add summed structure to df\n",
    "            progeny = []; counts = []; voxels = []\n",
    "            get_progeny(ontology_dict, struct, progeny)\n",
    "            for progen in progeny:\n",
    "                #first counts\n",
    "                try:\n",
    "                    counts.append(df[df.name == progen][\"counts\"].values[0])\n",
    "                except:\n",
    "                    counts.append(0)\n",
    "                try: #then voxels\n",
    "                    voxels.append(df[df.name == progen][\"voxels_in_structure\"].values[0])\n",
    "                except:\n",
    "                    voxels.append(0)\n",
    "            df = df.append({\"name\": struct, \"counts\": np.sum(np.array(counts)),\n",
    "                            \"voxels_in_structure\": np.sum(np.array(voxels)), \"parent_name\": \"root\"}, \n",
    "                           ignore_index=True)\n",
    "            \n",
    "        #remove summed structures progeny after you've counted them\n",
    "        df = df[~df.name.isin(sum_str_progeny)]\n",
    "        #remove structures and their children you don't want to analyze\n",
    "        for soi in exclude:\n",
    "            df = df[df.name != soi]\n",
    "            progeny = []; get_progeny(ontology_dict, soi, progeny)\n",
    "            for progen in progeny:\n",
    "                df = df[df.name != progen]\n",
    "                \n",
    "        #formatting\n",
    "        df[\"Brain\"] = nm\n",
    "        df[\"Condition\"] = conditions[nm]     \n",
    "        #append to composite dataframe\n",
    "        bglst.append(df)\n",
    "\n",
    "    df = pd.concat(bglst)\n",
    "    df[\"counts\"] = df[\"counts\"].apply(int)\n",
    "    df = df.drop(columns = [\"Unnamed: 0\"])\n",
    "    #get all brain names\n",
    "    brains = np.unique(df.Brain.values) \n",
    "    #save total counts in dict\n",
    "    total_counts = {}\n",
    "    percents = {}\n",
    "    #for each brain, get total counts\n",
    "    for brain in brains:\n",
    "        total_counts[brain] = df[df.Brain == brain].counts.sum(0)               \n",
    "    percents = [df[df.Brain == brain].counts.apply(lambda x: (x/total_counts[brain])*100).astype(\"float64\") for brain in brains]\n",
    "    #concantenate together\n",
    "    df_percent = pd.concat(percents)\n",
    "    df[\"percent\"] = df_percent\n",
    "    df[\"density\"] = df[df.voxels_in_structure > 0].apply(lambda x:x.counts/(float(x.voxels_in_structure*(scale**3))), 1)\n",
    "    \n",
    "    #export\n",
    "    df.to_csv(os.path.join(dst, \"cell_counts_summed.csv\"), index = None)\n",
    "    \n",
    "    return os.path.join(dst, \"cell_counts_summed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run the function to export the composite dataframe with the summed structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run\n",
    "csv_sum_pth = generate_summed_data_frame(conditions, lst, pth, \"Annotated_counts\", exclude, ontology_file,\n",
    "                                    sum_str, sum_str_progeny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-do statistics and figures\n",
    "\n",
    "Then, we can repeat the same basic, group-wise statistics we did with the full structures list (one-way ANOVA with a Tukey test) and re-make our stripplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#assume measure and cutoff for tests is same as above\n",
    "df = pd.read_csv(csv_sum_pth, index_col = None)\n",
    "#init new dataframe    \n",
    "df_anova = pd.DataFrame()\n",
    "df_anova[\"name\"] = df.name.unique()\n",
    "df_anova[\"parent_name\"] = [df.loc[df.name == nm, \"parent_name\"].values[0]\n",
    "                           for nm in df_anova[\"name\"]]\n",
    "df_anova[\"tukeyhsd\"] = np.ones(len(df_anova))*np.nan\n",
    "\n",
    "#performs an ANOVA for each structure in our list\n",
    "for nm in np.unique(df.name.values): #only gets unique names\n",
    "    f, pval = f_oneway(df[(df.name == nm) & (df.Condition == \"male-male\")][measure].values, \n",
    "                 df[(df.name == nm) & (df.Condition == \"female-male\")][measure].values,\n",
    "                 df[(df.name == nm) & (df.Condition == \"male-female\")][measure].values)\n",
    "\n",
    "    df_anova.loc[(df_anova[\"name\"] == nm), \"anova_f\"] = f\n",
    "    df_anova.loc[(df_anova[\"name\"] == nm), \"anova_pval\"] = pval\n",
    "\n",
    "    #doing post hoc on significant structures\n",
    "    if pval < cutoff:\n",
    "        mc = MultiComparison(df[df.name == nm][measure].values, df[df.name == nm ].Condition.values)\n",
    "        result = mc.tukeyhsd(alpha=cutoff) \n",
    "        df_anova.loc[(df_anova[\"name\"] == nm), \"tukeyhsd\"] = np.min(result.pvalues)\n",
    "\n",
    "df_anova.to_csv(os.path.join(dst, \"one_way_anova_summed_structures_{}.csv\".format(measure)))\n",
    "\n",
    "#find structures with multiple comparisons corrections    \n",
    "sig_structs = df_anova[(df_anova.anova_pval < cutoff) & \n",
    "                        (df_anova.tukeyhsd < cutoff)].name.values\n",
    "#plot the ones with lower density separately\n",
    "sig_structures_ld = [xx for xx in sig_structs \n",
    "                     if df[df.name == xx][measure].mean() < 100]\n",
    "\n",
    "df_nm = df[(df.name.isin(sig_structs)) & (~df.name.isin(sig_structures_ld))]\n",
    "fig, ax = plt.subplots(figsize=(5,10))\n",
    "sns.stripplot(y = \"name\", x = measure, hue = \"Condition\", data = df_nm,\n",
    "            orient = \"h\", size=5, palette=[\"black\", \"red\", \"blue\"])\n",
    "\n",
    "plt.xlabel(\"Density (cells/mm$^3$)\")\n",
    "plt.ylabel(\"Structure\")\n",
    "#save out\n",
    "plt.savefig(os.path.join(dst, \"boxplots_summed_structures_%s.pdf\" % measure), dpi = 300, bbox_inches = \"tight\")\n",
    "\n",
    "#plot the ones with lower density\n",
    "df_ld = df[df.name.isin(sig_structures_ld)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,2))\n",
    "sns.stripplot(y = \"name\", x = measure, hue = \"Condition\", data = df_ld,\n",
    "            orient = \"h\", size=5, palette=[\"black\", \"red\", \"blue\"])\n",
    "\n",
    "plt.xlabel(\"Density (cells/mm$^3$)\")\n",
    "plt.ylabel(\"Structure\")\n",
    "#save out\n",
    "plt.savefig(os.path.join(dst, \"boxplots_summed_structures_lower_%s.pdf\") % measure, \n",
    "            dpi = 300, bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
